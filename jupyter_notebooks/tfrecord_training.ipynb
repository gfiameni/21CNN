{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS: Namespace(HyperparameterIndex=0, LR_correction=True, N_noise=10, N_slice=4, N_walker=10000, X_fstring='lightcone_depthMhz_0_walker_{:04d}_slice_{:d}_seed_{:d}', X_shape=(25, 25, 526), data_fstring='/scratch/d.prelogovic/data/SKA_1000_tfrecord/{}_seed_{:d}_{:03d}_of_{:03d}', epochs=10, file_prefix='', gpus=1, logs_location='/scratch/d.prelogovic/runs/logs/', max_epochs=10, model=['playground', 'SummarySpace3D_simple'], model_type='RNN', noise_rolling=False, pTVT=[0.8, 0.1, 0.1], saving_location='/scratch/d.prelogovic/runs/models/', simple_run=False, tf=1, verbose=2, warmup=0, workers=24)\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "#define context, module with important variables\n",
    "###############################################################################\n",
    "from src.py21cnn import ctx\n",
    "ctx.init()\n",
    "###############################################################################\n",
    "#parsing inputs\n",
    "###############################################################################\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(prog = 'Large Database Model Run')\n",
    "\n",
    "parser.add_argument('--simple_run', type=int, choices=[0, 1], default = 0)\n",
    "parser.add_argument('--X_shape', type=str, default=\"25,25,526\")\n",
    "parser.add_argument('--N_walker', type=int, default=10000)\n",
    "parser.add_argument('--N_slice', type=int, default=4)\n",
    "parser.add_argument('--N_noise', type=int, default=10)\n",
    "parser.add_argument('--noise_rolling', type=int, choices=[0, 1], default = 0)\n",
    "parser.add_argument('--workers', type=int, default=24)\n",
    "parser.add_argument('--verbose', type=int, choices=[0, 1, 2], default=2)\n",
    "parser.add_argument('--X_fstring', type=str, default=\"lightcone_depthMhz_0_walker_{:04d}_slice_{:d}_seed_{:d}\")\n",
    "parser.add_argument('--data_fstring', type=str, default=\"/scratch/d.prelogovic/data/SKA_1000_tfrecord/{}_seed_{:d}_{:03d}_of_{:03d}\")\n",
    "parser.add_argument('--saving_location', type=str, default=\"/scratch/d.prelogovic/runs/models/\")\n",
    "parser.add_argument('--logs_location', type=str, default=\"/scratch/d.prelogovic/runs/logs/\")\n",
    "parser.add_argument('--model', type=str, default=\"playground.SummarySpace3D_simple\")\n",
    "parser.add_argument('--model_type', type=str, default=\"RNN\")\n",
    "parser.add_argument('--HyperparameterIndex', type=int, default=0)\n",
    "parser.add_argument('--epochs', type=int, default=10)\n",
    "parser.add_argument('--max_epochs', type=int, default=-1)\n",
    "parser.add_argument('--gpus', type=int, default=1)\n",
    "parser.add_argument('--LR_correction', type=int, choices=[0, 1], default=1)\n",
    "parser.add_argument('--file_prefix', type=str, default=\"\")\n",
    "# parser.add_argument('--patience', type=int, default=10)\n",
    "parser.add_argument('--pTVT', type=str, default = \"0.8,0.1,0.1\")\n",
    "parser.add_argument('--warmup', type=int, default=0)\n",
    "parser.add_argument('--tf', type = int, choices = [1, 2], default = 1)\n",
    "\n",
    "inputs = parser.parse_args(\"\")\n",
    "inputs.LR_correction = bool(inputs.LR_correction)\n",
    "inputs.simple_run = bool(inputs.simple_run)\n",
    "inputs.noise_rolling = bool(inputs.noise_rolling)\n",
    "inputs.pTVT = [float(i) for i in inputs.pTVT.split(',')]\n",
    "inputs.model = inputs.model.split('.')\n",
    "inputs.X_shape = tuple([int(i) for i in inputs.X_shape.split(',')])\n",
    "if len(inputs.model_type) == 0:\n",
    "    inputs.model_type = inputs.model[0]\n",
    "if inputs.max_epochs == -1:\n",
    "    inputs.max_epochs = inputs.epochs\n",
    "elif inputs.max_epochs < inputs.epochs:\n",
    "    raise ValueError(\"epochs shouldn't be larger than max_epochs\")\n",
    "\n",
    "print(\"INPUTS:\", inputs)\n",
    "ctx.inputs = inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/d.prelogovic/opt/miniconda3/envs/tf1/lib/python3.6/site-packages/horovod/tensorflow/__init__.py:163: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/d.prelogovic/opt/miniconda3/envs/tf1/lib/python3.6/site-packages/horovod/tensorflow/__init__.py:189: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "#seting up GPUs\n",
    "###############################################################################\n",
    "import tensorflow as tf\n",
    "import horovod.tensorflow.keras as hvd\n",
    "if ctx.inputs.tf == 1:\n",
    "    # tf.compat.v1.enable_eager_execution()\n",
    "    if ctx.inputs.gpus == 1:\n",
    "        # #setting up GPU\n",
    "        config = tf.compat.v1.ConfigProto()\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = 1. #setting the percentage of GPU usage\n",
    "        config.gpu_options.visible_device_list = \"0\" #for picking only some devices\n",
    "        config.gpu_options.allow_growth = True\n",
    "        # config.log_device_placement=True\n",
    "#         tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\n",
    "        tf.compat.v1.enable_eager_execution(config=config)\n",
    "    elif ctx.inputs.gpus > 1:\n",
    "        #init Horovod\n",
    "        hvd.init()\n",
    "        # Horovod: pin GPU to be used to process local rank (one GPU per process)\n",
    "        config = tf.compat.v1.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.gpu_options.visible_device_list = str(hvd.local_rank())\n",
    "        tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\n",
    "        # tf.compat.v1.enable_eager_execution(config=config)\n",
    "    else:\n",
    "        raise ValueError('number of gpus shoud be > 0')\n",
    "else:\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    if ctx.inputs.gpus > 1:\n",
    "        hvd.init()\n",
    "        tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n",
    "\n",
    "    # #assuming each node has one gpu, for other configurations, has to be properly modified\n",
    "    # #gpus has only one member\n",
    "    # gpu = gpus[0]\n",
    "    # tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    # if ctx.inputs.gpus > 1:\n",
    "    #     hvd.init()\n",
    "    # tf.config.experimental.set_visible_devices(gpu, \"GPU\")\n",
    "\n",
    "#importing keras at the end, I had some issues if I import it before setting GPUs\n",
    "from tensorflow import keras\n",
    "keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "if ctx.inputs.gpus > 1:\n",
    "    print(\"HVD.SIZE\", hvd.size())\n",
    "\n",
    "if ctx.inputs.gpus > 1:\n",
    "    ctx.main_process = True if hvd.rank() == 0 else False\n",
    "else:\n",
    "    ctx.main_process = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/d.prelogovic/21CNN/src/py21cnn/utilities.py:181: The name tf.keras.initializers.he_uniform is deprecated. Please use tf.compat.v1.keras.initializers.he_uniform instead.\n",
      "\n",
      "HYPERPARAMETERS: Loss:mse__Optimizer:Adam__LR:0.0010000000__Activation:relu__BN:True__dropout:0.20__reduceLR:True__Batch:00020__Epochs:00010__NoiseRolling:False\n",
      "c499ab7d84c5b92a5928da1a728918d3\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "#seting hyperparameters\n",
    "###############################################################################\n",
    "import copy\n",
    "import itertools\n",
    "import sys\n",
    "from src.py21cnn import utilities\n",
    "from src.py21cnn import hyperparameters\n",
    "if ctx.inputs.simple_run == True:\n",
    "    HP_dict = hyperparameters.HP_simple()\n",
    "else:\n",
    "    HP = hyperparameters.HP()\n",
    "    HP_list = list(itertools.product(*HP.values()))\n",
    "    HP_dict = dict(zip(HP.keys(), HP_list[ctx.inputs.HyperparameterIndex]))    \n",
    "\n",
    "#correct learning rate for multigpu run\n",
    "if ctx.inputs.LR_correction == True:\n",
    "        HP_dict[\"LearningRate\"] *= ctx.inputs.gpus\n",
    "        \n",
    "HP = utilities.AuxiliaryHyperparameters(\n",
    "    model_name=f\"{ctx.inputs.model[0]}_{ctx.inputs.model[1]}\", \n",
    "    Epochs=ctx.inputs.epochs, \n",
    "    MaxEpochs=ctx.inputs.max_epochs, \n",
    "    NoiseRolling=ctx.inputs.noise_rolling,\n",
    "    **HP_dict,\n",
    "    )\n",
    "\n",
    "print(\"HYPERPARAMETERS:\", str(HP))\n",
    "ctx.HP = HP\n",
    "print(ctx.HP.hash())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(serialized_example, model_type):\n",
    "    \"\"\"Parses an image and label from the given `serialized_example`.\"\"\"\n",
    "    print(serialized_example)\n",
    "    example = tf.io.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'Xx': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'Xy': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'Xz': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'X': tf.io.FixedLenFeature([], tf.string),\n",
    "            'Yx': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'Y': tf.io.FixedLenFeature([], tf.string),\n",
    "        })\n",
    "    xx = tf.cast(example['Xx'], tf.int64)\n",
    "    xy = tf.cast(example['Xy'], tf.int64)\n",
    "    xz = tf.cast(example['Xz'], tf.int64)\n",
    "    x = tf.io.decode_raw(example['X'], tf.float32)\n",
    "    x = tf.reshape(x, (xx, xy, xz))\n",
    "    if model_type == \"RNN\":\n",
    "        x = tf.transpose(x)\n",
    "    x = tf.expand_dims(x, -1)\n",
    "    yx = tf.cast(example['Yx'], tf.int64)\n",
    "    y = tf.io.decode_raw(example['Y'], tf.float32)\n",
    "    y = tf.reshape(y, (yx,))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shards(filenames, shuffle = True):\n",
    "    shards = tf.data.Dataset.from_tensor_slices(filenames[0])\n",
    "    if shuffle == True:\n",
    "        shards = shards.shuffle(len(filenames[0]))\n",
    "    for i in range(1, len(filenames)):\n",
    "        t_shards = tf.data.Dataset.from_tensor_slices(filenames[i])\n",
    "        if shuffle == True:\n",
    "            t_shards = t_shards.shuffle(len(filenames[i]))\n",
    "        shards = shards.concatenate(t_shards)\n",
    "    shards = shards.repeat()\n",
    "    return shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(filenames, model_type, shuffle, batch_size, buffer_size, workers):\n",
    "    \"\"\"Read TFRecords files and turn them into a TFRecordDataset.\"\"\"\n",
    "    shards = create_shards(filenames)\n",
    "    if shuffle == True:\n",
    "        dataset = shards.interleave(tf.data.TFRecordDataset, cycle_length = 5, block_length = 1)\n",
    "    else:\n",
    "        dataset = shards.interleave(tf.data.TFRecordDataset, cycle_length = 1, block_length = 1)\n",
    "    #   dataset = dataset.shuffle(buffer_size=8192)\n",
    "    dataset = dataset.map(map_func = lambda x: decode(x, model_type), num_parallel_calls = workers).batch(batch_size = batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size = buffer_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filenames(N_noise, shardsTVT = {\"train\": 320, \"validation\": 40, \"test\": 40}):\n",
    "    filenames = {\n",
    "        \"train\": [],\n",
    "        \"validation\": [],\n",
    "        \"test\": []\n",
    "        }\n",
    "    for key in filenames.keys():\n",
    "        for seed in range(N_noise):\n",
    "            filenames[key].append([inputs.data_fstring.format(key, seed, i, shardsTVT[key]-1) for i in range(shardsTVT[key])])\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"args_0:0\", shape=(), dtype=string)\n",
      "Tensor(\"args_0:0\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "filenames = create_filenames(N_noise = 10)\n",
    "train_ds = get_dataset(filenames[\"train\"], ctx.inputs.model_type, shuffle = True, batch_size = 20, buffer_size = 16, workers = 3)\n",
    "validation_ds = get_dataset([filenames[\"validation\"][0]], ctx.inputs.model_type, shuffle = False, batch_size = 20, buffer_size = 16, workers = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.py21cnn import utilities\n",
    "from src.py21cnn.formatting import Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "class LargeData:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimensionality = 2,\n",
    "        removed_average = True,\n",
    "        normalized = True,\n",
    "        Zmax = 30,\n",
    "        filetype = 'float32',\n",
    "        formatting = [],\n",
    "        noise = [\"tools21cm\", \"SKA1000\"],\n",
    "        shape = None,\n",
    "        load_all = False,\n",
    "        ):\n",
    "        self.dimensionality = dimensionality\n",
    "        self.removed_average = removed_average\n",
    "        self.normalized = normalized\n",
    "        self.Zmax = Zmax\n",
    "        self.filetype = filetype\n",
    "        if len(formatting) == 0:\n",
    "            default_formatting = ['clipped_-250_+50', 'NaN_removed', 'TVT_parameterwise']\n",
    "            if self.dimensionality == 2:\n",
    "                default_formatting.append('boxcar44')\n",
    "                default_formatting.append('10slices')\n",
    "            if self.dimensionality == 3:\n",
    "                default_formatting.append('boxcar444')\n",
    "                default_formatting.append('sliced22')\n",
    "            # default_formatting.sort()\n",
    "            self.formatting = default_formatting\n",
    "        else:\n",
    "            # formatting.sort()\n",
    "            self.formatting = formatting\n",
    "        self.noise = noise + [f\"walkers_{ctx.inputs.N_walker}\", f\"slices_{ctx.inputs.N_slice}\", f\"noise_{ctx.inputs.N_noise}\"]\n",
    "        self.shape = shape\n",
    "        self.load_all = load_all\n",
    "        self.load()\n",
    "\n",
    "    def __str__(self):\n",
    "        self.formatting.sort()\n",
    "        self.noise.sort()\n",
    "        S = f\"dim:{self.dimensionality}__removed_average:{self.removed_average}__normalized:{self.normalized}__Zmax:{self.Zmax}__dtype:{self.filetype}\"\n",
    "        for i in self.formatting:\n",
    "            S += f\"__{i}\"\n",
    "        for i in self.noise:\n",
    "            S += f\"__{i}\"\n",
    "        return S\n",
    "    \n",
    "    def hash(self):\n",
    "        return hashlib.md5(self.__str__().encode()).hexdigest()\n",
    "\n",
    "    def load(self):\n",
    "        permutation = Filters.constructIndexArray(ctx.inputs.N_walker, *ctx.inputs.pTVT, 1312)\n",
    "        # print(permutation)\n",
    "        self.partition = {\n",
    "            \"train\": [], \n",
    "            \"validation\": [], \n",
    "            \"test\": []}\n",
    "        self.noise_rolling_partition = {\n",
    "            \"train\": [], \n",
    "            \"validation\": [], \n",
    "            \"test\": []}\n",
    "        keys = list(self.partition.keys())\n",
    "        for key in keys:\n",
    "            for seed in range(ctx.inputs.N_noise):\n",
    "                self.noise_rolling_partition[key].append([])\n",
    "        self.inputs = {}\n",
    "        for walker in range(ctx.inputs.N_walker):\n",
    "            for s in range(ctx.inputs.N_slice):\n",
    "                for seed in range(ctx.inputs.N_noise):\n",
    "                    ID = ctx.inputs.X_fstring.format(walker, s, seed)\n",
    "                    self.partition[keys[permutation[walker]]].append(ID)\n",
    "                    self.noise_rolling_partition[keys[permutation[walker]]][seed].append(ID)\n",
    "                    if self.load_all == True:\n",
    "                        self.inputs[ID] = np.load(f\"{ctx.inputs.data_location}{ID}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b515f26321d231c6b06f7c086ce3e097\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "#constructing TVT partitions of the data and assigning labels\n",
    "###############################################################################\n",
    "data_shape = ctx.inputs.X_shape[::-1] + (1,) if ctx.inputs.model_type == \"RNN\" else ctx.inputs.X_shape + (1,)\n",
    "ctx.Data = LargeData(dimensionality = 3, shape = data_shape, load_all = False)\n",
    "print(ctx.Data.hash())\n",
    "ctx.filepath = f\"{ctx.inputs.saving_location}{ctx.inputs.file_prefix}{ctx.inputs.model[0]}_{ctx.inputs.model[1]}_{ctx.HP.hash()}_{ctx.Data.hash()}\"\n",
    "ctx.logdir = f\"{ctx.inputs.logs_location}{ctx.inputs.file_prefix}{ctx.inputs.model[0]}/{ctx.inputs.model[1]}/{ctx.Data.hash()}/{ctx.HP.hash()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"playground_SummarySpace3D_simple\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 526, 25, 25, 1)]  0         \n",
      "_________________________________________________________________\n",
      "conv1 (TimeDistributed)      (None, 526, 18, 18, 64)   4160      \n",
      "_________________________________________________________________\n",
      "maxpool (TimeDistributed)    (None, 526, 64)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 526, 64)           256       \n",
      "_________________________________________________________________\n",
      "dropout (TimeDistributed)    (None, 526, 64)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm (CuDNNLSTM)       (None, 16)                5248      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 9,732\n",
      "Trainable params: 9,604\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "ModelClassObject = getattr(importlib.import_module(f'src.py21cnn.architectures.{ctx.inputs.model[0]}'), ctx.inputs.model[1])\n",
    "ModelClass = ModelClassObject(data_shape, HP)\n",
    "ModelClass.build()\n",
    "\n",
    "ctx.model = ModelClass.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.file = open(self.filename, 'a')\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.file.write(f\"{time.time() - self.epoch_time_start}\\n\")\n",
    "        self.file.flush()\n",
    "        os.fsync(self.file.fileno())\n",
    "    def on_train_end(self, logs={}):\n",
    "        self.file.close()\n",
    "\n",
    "\n",
    "class LR_tracer(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        lr = keras.backend.eval( self.model.optimizer.lr )\n",
    "        print(f\"LR: {lr:.10f}\")\n",
    "\n",
    "\n",
    "class LR_scheduler:\n",
    "    def __init__(self, total_epochs, inital_LR, multi_gpu_run = False, reduce_factor = 0.1):\n",
    "        self.total_epochs = total_epochs\n",
    "        self.initial_LR = inital_LR\n",
    "        self.multi_gpu_run = multi_gpu_run\n",
    "        self.reduce_factor = reduce_factor\n",
    "    def scheduler(self, epoch):\n",
    "        \"\"\"\n",
    "        Returns learning rate at a given epoch. \n",
    "        Recieves total number of epochs and initial learning rate\n",
    "        \"\"\"\n",
    "        # print(f\"IN LR_scheduler, initLR {self.initial_LR}, epoch {epoch}, frac. {(epoch + 1) / self.total_epochs}\")\n",
    "        if (epoch + 1) / self.total_epochs < 0.5:\n",
    "            return self.initial_LR\n",
    "        elif (epoch + 1) / self.total_epochs < 0.75:\n",
    "            return self.initial_LR * self.reduce_factor\n",
    "        else:\n",
    "            return self.initial_LR * self.reduce_factor ** 2\n",
    "    def callback(self):\n",
    "        if self.multi_gpu_run == True:\n",
    "            return hvd.callbacks.LearningRateScheduleCallback(self.scheduler)\n",
    "        else:\n",
    "            return tf.keras.callbacks.LearningRateScheduler(self.scheduler)\n",
    "\n",
    "\n",
    "def R2(y_true, y_pred):\n",
    "        SS_res = keras.backend.sum(keras.backend.square(y_true-y_pred)) \n",
    "        SS_tot = keras.backend.sum(keras.backend.square(y_true - keras.backend.mean(y_true, axis=0)))\n",
    "        return (1 - SS_res/(SS_tot + keras.backend.epsilon()))\n",
    "def R2_numpy(y_true, y_pred):\n",
    "        SS_res = np.sum((y_true - y_pred)**2) \n",
    "        SS_tot = np.sum((y_true - np.mean(y_true))**2)\n",
    "        return (1 - SS_res/(SS_tot + 1e-7))\n",
    "def R2_final(y_true, y_pred):\n",
    "        SS_res = np.sum((y_true - y_pred)**2)\n",
    "        SS_tot = np.sum((y_true - np.mean(y_true, axis=0))**2)\n",
    "        return (1 - SS_res/(SS_tot + 1e-7))\n",
    "\n",
    "\n",
    "def define_callbacks():\n",
    "    if ctx.inputs.gpus == 1:\n",
    "        saving_callbacks = True\n",
    "        horovod_callbacks = False\n",
    "    else:\n",
    "        saving_callbacks = True if hvd.rank() == 0 else False\n",
    "        horovod_callbacks = True\n",
    "\n",
    "    if saving_callbacks == True:\n",
    "        saving_callbacks = [\n",
    "            # keras.callbacks.TensorBoard(ctx.logdir, update_freq=\"epoch\"),\n",
    "            # hp.KerasCallback(logdir, HP_TensorBoard),\n",
    "            TimeHistory(f\"{ctx.filepath}_time.txt\"),\n",
    "            keras.callbacks.ModelCheckpoint(f\"{ctx.filepath}_best.hdf5\", monitor='val_loss', save_best_only=True, verbose=True),\n",
    "            keras.callbacks.ModelCheckpoint(f\"{ctx.filepath}_last.hdf5\", monitor='val_loss', save_best_only=False, verbose=True), \n",
    "            keras.callbacks.CSVLogger(f\"{ctx.filepath}.log\", separator=',', append=True),\n",
    "            LR_tracer(),\n",
    "            ]\n",
    "    else:\n",
    "        saving_callbacks = []\n",
    "    if horovod_callbacks == True:\n",
    "        horovod_callbacks = [\n",
    "            hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n",
    "            hvd.callbacks.MetricAverageCallback(),\n",
    "            ]\n",
    "        if ctx.load_model == False:\n",
    "            horovod_callbacks.append(hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=ctx.inputs.warmup))\n",
    "    else:\n",
    "        horovod_callbacks = []\n",
    "    \n",
    "    important_callbacks = [\n",
    "        keras.callbacks.TerminateOnNaN(),\n",
    "        ]\n",
    "    if ctx.HP.ReducingLR == True:\n",
    "        scheduler = LR_scheduler(ctx.HP.MaxEpochs, ctx.HP.LearningRate, reduce_factor = 0.1)\n",
    "        important_callbacks.append(scheduler.callback())\n",
    "\n",
    "    #not saving into ctx as it might screw up things during broadcast of variables\n",
    "    return horovod_callbacks + saving_callbacks + important_callbacks\n",
    "\n",
    "\n",
    "def define_model(restore_training):\n",
    "    model_exists = os.path.exists(f\"{ctx.filepath}_last.hdf5\")\n",
    "    #define in what case to load the model\n",
    "    # if model_exists == True and restore_training == True:\n",
    "    #     if ctx.inputs.gpus == 1:\n",
    "    #         load_model = True\n",
    "    #         load_function = keras.models.load_model\n",
    "    #     else:\n",
    "    #         if hvd.rank() == 0:\n",
    "    #             load_model = True\n",
    "    #             load_function = hvd.load_model\n",
    "    #         else:\n",
    "    #             load_model = False\n",
    "    # else:\n",
    "    #     load_model = False\n",
    "    if model_exists == True and restore_training == True:\n",
    "        load_model = True\n",
    "        if ctx.inputs.gpus == 1:\n",
    "            load_function = keras.models.load_model\n",
    "        else:\n",
    "            load_function = hvd.load_model\n",
    "    else:\n",
    "        load_model = False\n",
    "    ctx.load_model = load_model\n",
    "\n",
    "    #determine steps_per_epoch\n",
    "    if isinstance(ctx.Data, Data):\n",
    "        steps_per_epoch = ctx.Data.X[\"train\"].shape[0] // ctx.inputs.gpus // ctx.HP.BatchSize\n",
    "        validation_steps = ctx.Data.X[\"val\"].shape[0] // ctx.HP.BatchSize\n",
    "    elif isinstance(ctx.Data, LargeData):\n",
    "        if ctx.inputs.noise_rolling == True:\n",
    "            steps_per_epoch = len(ctx.Data.noise_rolling_partition[\"train\"][0]) // ctx.inputs.gpus // ctx.HP.BatchSize\n",
    "            validation_steps = len(ctx.Data.noise_rolling_partition[\"validation\"][0]) // ctx.HP.BatchSize   \n",
    "        else:         \n",
    "            steps_per_epoch = len(ctx.Data.partition[\"train\"]) // ctx.inputs.gpus // ctx.HP.BatchSize\n",
    "            validation_steps = len(ctx.Data.partition[\"validation\"]) // ctx.HP.BatchSize\n",
    "    else:\n",
    "        raise TypeError(\"ctx.Data should be an instance of {Data, LargeData} class\")\n",
    "\n",
    "    if validation_steps == 0:\n",
    "        raise ValueError(\"Number of validation steps per epoch is 0. Change batch size, validation probability, or give me more data.\")\n",
    "\n",
    "    #load the model\n",
    "    if load_model == True:\n",
    "        custom_obj = {}\n",
    "        custom_obj[\"R2\"] = R2\n",
    "        if ctx.HP.ActivationFunction[0] == \"leakyrelu\":\n",
    "            custom_obj[ctx.HP.ActivationFunction[0]] = ctx.HP.ActivationFunction[1][\"activation\"]\n",
    "        #if loading last model fails for some reason, load the best one\n",
    "        try:\n",
    "            ctx.model = load_function(f\"{ctx.filepath}_last.hdf5\", custom_objects=custom_obj)\n",
    "        except:\n",
    "            ctx.model = load_function(f\"{ctx.filepath}_best.hdf5\", custom_objects=custom_obj)\n",
    "\n",
    "        with open(f\"{ctx.filepath}.log\") as f:\n",
    "            number_of_epochs_trained = len(f.readlines()) - 1  #the first line is description\n",
    "            print(\"NUMBER_OF_EPOCHS_TRAINED\", number_of_epochs_trained)\n",
    "        if ctx.HP.Epochs + number_of_epochs_trained > ctx.HP.MaxEpochs:\n",
    "            final_epochs = ctx.HP.MaxEpochs\n",
    "        else:\n",
    "            final_epochs = ctx.HP.Epochs + number_of_epochs_trained\n",
    "\n",
    "        ctx.fit_options = {\n",
    "            \"epochs\": final_epochs,\n",
    "            \"initial_epoch\": number_of_epochs_trained,\n",
    "            \"steps_per_epoch\": steps_per_epoch,\n",
    "            \"validation_steps\": validation_steps,\n",
    "            }\n",
    "        ctx.compile_options = {}\n",
    "    else:\n",
    "        ctx.fit_options = {\n",
    "            \"epochs\": ctx.HP.Epochs,\n",
    "            \"initial_epoch\": 0,\n",
    "            \"steps_per_epoch\": steps_per_epoch,\n",
    "            \"validation_steps\": validation_steps,\n",
    "            }\n",
    "        ctx.compile_options = {\n",
    "            \"loss\": ctx.HP.Loss[1],\n",
    "            \"optimizer\": ctx.HP.Optimizer[0](**ctx.HP.Optimizer[2]),\n",
    "            \"metrics\": [R2],\n",
    "            }\n",
    "        if ctx.inputs.gpus > 1:\n",
    "            ctx.compile_options[\"optimizer\"] = hvd.DistributedOptimizer(ctx.compile_options[\"optimizer\"])\n",
    "\n",
    "\n",
    "callbacks = define_callbacks()\n",
    "define_model(restore_training = False)\n",
    "if len(ctx.compile_options) > 0:\n",
    "    ctx.model.compile(**ctx.compile_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 'mse', 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7fe480a18438>, 'metrics': [<function R2 at 0x7fe30c6379d8>]}\n",
      "{'epochs': 10, 'initial_epoch': 0, 'steps_per_epoch': 16000, 'validation_steps': 2000}\n"
     ]
    }
   ],
   "source": [
    "print(ctx.compile_options)\n",
    "print(ctx.fit_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 20 steps, validate for 10 steps\n",
      "LR: 0.0010000000\n",
      "Epoch 1/3\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.0453 - R2: -0.1192\n",
      "Epoch 00001: val_loss improved from inf to 1.03812, saving model to /scratch/d.prelogovic/runs/models/playground_SummarySpace3D_simple_c499ab7d84c5b92a5928da1a728918d3_b515f26321d231c6b06f7c086ce3e097_best.hdf5\n",
      "\n",
      "Epoch 00001: saving model to /scratch/d.prelogovic/runs/models/playground_SummarySpace3D_simple_c499ab7d84c5b92a5928da1a728918d3_b515f26321d231c6b06f7c086ce3e097_last.hdf5\n",
      "20/20 [==============================] - 7s 348ms/step - loss: 1.0539 - R2: -0.1196 - val_loss: 1.0381 - val_R2: -0.0676\n",
      "LR: 0.0010000000\n",
      "Epoch 2/3\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.0376 - R2: -0.0853\n",
      "Epoch 00002: val_loss improved from 1.03812 to 1.02451, saving model to /scratch/d.prelogovic/runs/models/playground_SummarySpace3D_simple_c499ab7d84c5b92a5928da1a728918d3_b515f26321d231c6b06f7c086ce3e097_best.hdf5\n",
      "\n",
      "Epoch 00002: saving model to /scratch/d.prelogovic/runs/models/playground_SummarySpace3D_simple_c499ab7d84c5b92a5928da1a728918d3_b515f26321d231c6b06f7c086ce3e097_last.hdf5\n",
      "20/20 [==============================] - 2s 76ms/step - loss: 1.0440 - R2: -0.0871 - val_loss: 1.0245 - val_R2: -0.0536\n",
      "LR: 0.0010000000\n",
      "Epoch 3/3\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.0107 - R2: -0.0603\n",
      "Epoch 00003: val_loss improved from 1.02451 to 1.01975, saving model to /scratch/d.prelogovic/runs/models/playground_SummarySpace3D_simple_c499ab7d84c5b92a5928da1a728918d3_b515f26321d231c6b06f7c086ce3e097_best.hdf5\n",
      "\n",
      "Epoch 00003: saving model to /scratch/d.prelogovic/runs/models/playground_SummarySpace3D_simple_c499ab7d84c5b92a5928da1a728918d3_b515f26321d231c6b06f7c086ce3e097_last.hdf5\n",
      "20/20 [==============================] - 2s 78ms/step - loss: 1.0070 - R2: -0.0572 - val_loss: 1.0198 - val_R2: -0.0489\n"
     ]
    }
   ],
   "source": [
    "history = ctx.model.fit(\n",
    "    train_ds,\n",
    "    validation_data = validation_ds,\n",
    "    callbacks = callbacks,\n",
    "    epochs = 3,\n",
    "    steps_per_epoch = 20,\n",
    "    validation_steps = 10,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': None,\n",
       " 'epochs': 3,\n",
       " 'steps': 20,\n",
       " 'samples': 20,\n",
       " 'verbose': 0,\n",
       " 'do_validation': True,\n",
       " 'metrics': ['loss', 'R2', 'val_loss', 'val_R2']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.params\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
